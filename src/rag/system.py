import os
from typing import List, Dict, Any
from pinecone import Pinecone
import openai

from ..utils.config import get_config, DEBUG_MODE

class InsuranceRAGSystem:
    """ë³´í—˜ ì•½ê´€ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # ì„¤ì • ë¡œë“œ
        self.config = get_config()
        
        # Pinecone ì´ˆê¸°í™”
        self.pc = Pinecone(api_key=self.config["pinecone_api_key"])
        
        # ì¸ë±ìŠ¤ ì—°ê²°
        index_name = self.config["pinecone_index_name"]
        index_description = self.pc.describe_index(index_name)
        self.index = self.pc.Index(host=index_description.host)
        
        # OpenAI ì´ˆê¸°í™”
        self.client = openai.OpenAI(api_key=self.config["openai_api_key"])
        
        print("âœ… RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def search_relevant_chunks(self, query: str, top_k: int = 5, namespace: str = "default") -> List[Dict]:
        """
        ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ì²­í¬ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        """
        try:
            # Pineconeì˜ search_records ì‚¬ìš© (integrated inference)
            from pinecone import SearchQuery
            
            response = self.index.search_records(
                namespace=namespace,
                query=SearchQuery(
                    inputs={
                        "text": query,  # fieldMapì˜ "text" í•„ë“œ ì‚¬ìš©
                    },
                    top_k=top_k
                )
            )
            
            # ë””ë²„ê·¸ ëª¨ë“œì—ì„œë§Œ ì¶œë ¥
            if DEBUG_MODE:
                print(f"ê²€ìƒ‰ ì‘ë‹µ íƒ€ì…: {type(response)}")
                print(f"ì‘ë‹µ ë‚´ìš©: {response}")
            
            # ê²°ê³¼ ì²˜ë¦¬
            results = []
            
            # Pinecone ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
            if hasattr(response, 'result') and hasattr(response.result, 'hits'):
                hits = response.result.hits
                for hit in hits:
                    # fields êµ¬ì¡°ì—ì„œ ë°ì´í„° ì¶”ì¶œ
                    fields = hit.fields
                    result = {
                        'id': hit._id,
                        'score': hit._score,
                        'content': fields.get('text', ''),  # text í•„ë“œì—ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                        'source': fields.get('source', 'ë³´í—˜ì•½ê´€'),
                        'chunk_index': int(fields.get('chunk_index', 0)),
                        'chunk_size': int(fields.get('chunk_size', 0))
                    }
                    results.append(result)
            else:
                print("ì˜ˆìƒí•˜ì§€ ëª»í•œ ì‘ë‹µ êµ¬ì¡°ì…ë‹ˆë‹¤.")
                print(f"ì‘ë‹µ ê°ì²´ ì†ì„±: {dir(response)}")
            
            if DEBUG_MODE:
                print(f"ğŸ“„ {len(results)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return results
            
        except Exception as e:
            print(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def generate_answer(self, query: str, contexts: List[Dict], max_context_length: int = None) -> str:
        """
        ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        if max_context_length is None:
            max_context_length = self.config["max_context_length"]
            
        try:
            # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            context_text = ""
            for i, ctx in enumerate(contexts[:3]):  # ìƒìœ„ 3ê°œë§Œ ì‚¬ìš©
                content = ctx.get('content', '')[:1000]  # ê° ì²­í¬ë‹¹ ìµœëŒ€ 1000ì
                context_text += f"[ì°¸ê³ ìë£Œ {i+1}]\n{content}\n\n"
            
            if len(context_text) > max_context_length:
                context_text = context_text[:max_context_length] + "..."
            
            if DEBUG_MODE:
                print(f"context_text: {context_text}")

            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            system_prompt = """ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë³´í—˜ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. 
ì œê³µëœ LIGì†í•´ë³´í—˜ ì•½ê´€ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹µë³€ ì§€ì¹¨:
1. ì œê³µëœ ì°¸ê³ ìë£Œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”
2. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš”  
3. êµ¬ì²´ì ì¸ ì¡°í•­ì´ë‚˜ ì ˆì°¨ê°€ ìˆë‹¤ë©´ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”
4. ë§Œì•½ ì œê³µëœ ìë£Œì—ì„œ ì •í™•í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, ê·¸ ì ì„ ëª…ì‹œí•˜ê³  ë³´í—˜íšŒì‚¬ì— ì§ì ‘ ë¬¸ì˜í•˜ë„ë¡ ì•ˆë‚´í•˜ì„¸ìš”
5. ë‹µë³€ì€ 3-4ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”"""

            user_prompt = f"""ë‹¤ìŒ LIGì†í•´ë³´í—˜ ì•½ê´€ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

{context_text}

ì§ˆë¬¸: {query}

ë‹µë³€:"""
            
            # OpenAI API í˜¸ì¶œ
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=500,
                temperature=0.1
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"OpenAI API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            
            # OpenAI API ì‹¤íŒ¨ ì‹œ í´ë°± ë‹µë³€
            if contexts:
                first_context = contexts[0].get('content', '')[:500]
                return f"ê²€ìƒ‰ëœ ì•½ê´€ ë‚´ìš©ì— ë”°ë¥´ë©´: {first_context}... ë” êµ¬ì²´ì ì¸ ì •ë³´ëŠ” ë³´í—˜íšŒì‚¬ì— ì§ì ‘ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
            
            return "í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë³´í—˜íšŒì‚¬ì— ì§ì ‘ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
    
    def ask(self, query: str) -> Dict[str, Any]:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        if DEBUG_MODE:
            print(f"ğŸ” ì§ˆë¬¸: {query}")
        
        # 1. ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
        relevant_chunks = self.search_relevant_chunks(query, top_k=self.config["max_search_results"])
        if DEBUG_MODE:
            print(f"ğŸ“„ {len(relevant_chunks)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        if not relevant_chunks:
            return {
                'answer': 'ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ë³´í—˜ ì•½ê´€ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'sources': [],
                'query': query
            }
        
        # 2. ë‹µë³€ ìƒì„±
        answer = self.generate_answer(query, relevant_chunks)
        
        # 3. ì†ŒìŠ¤ ì •ë³´ ì¤€ë¹„
        sources = []
        for chunk in relevant_chunks[:3]:
            sources.append({
                'id': chunk.get('id', ''),
                'score': chunk.get('score', 0.0),
                'content': chunk.get('content', ''),
                'source': chunk.get('source', 'ë³´í—˜ì•½ê´€'),
                'chunk_index': chunk.get('chunk_index', 0),
                'chunk_size': chunk.get('chunk_size', 0)
            })
        
        return {
            'answer': answer,
            'sources': sources,
            'query': query
        }
